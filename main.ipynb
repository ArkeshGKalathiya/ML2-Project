{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiqJJnnfwefI",
        "outputId": "a9d1b323-7fc4-4388-c005-4cb13c3d40a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n",
            "zsh:1: command not found: gdown\n",
            "unzip:  cannot find or open jigsaw-toxic-comment-classification-challenge.zip, jigsaw-toxic-comment-classification-challenge.zip.zip or jigsaw-toxic-comment-classification-challenge.zip.ZIP.\n",
            "Archive:  train.csv.zip\n",
            "checkdir:  cannot create extraction directory: /content\n",
            "           Read-only file system\n",
            "Archive:  test.csv.zip\n",
            "checkdir:  cannot create extraction directory: /content\n",
            "           Read-only file system\n"
          ]
        }
      ],
      "source": [
        "! pip install gdown \n",
        "! gdown 1z28MmoZmBd46FRF4hmFkmLIFemqxg5C8\n",
        "! unzip -o jigsaw-toxic-comment-classification-challenge.zip\n",
        "! rm -f jigsaw-toxic-comment-classification-challenge.zip\n",
        "! unzip -o train.csv.zip -d /content/ \n",
        "! unzip -o test.csv.zip -d /content/ \n",
        "#https://drive.google.com/file/d/1z28MmoZmBd46FRF4hmFkmLIFemqxg5C8/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vVplPxaryqZ",
        "outputId": "f1667134-c572-49f9-fcc9-01c9dd61fc55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/arkeshkalathiya/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/arkeshkalathiya/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/arkeshkalathiya/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/arkeshkalathiya/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9hUT2ftWryqd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from gensim.utils import simple_preprocess\n",
        "import joblib\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import multilabel_confusion_matrix, classification_report, plot_roc_curve\n",
        "\n",
        "import gensim.downloader as trained_models\n",
        "import os.path as path\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV, GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # supress warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1ynKAGxZryqe",
        "outputId": "8bff4c1e-9ab4-4f9b-8dd0-cf191d7ee274"
      },
      "outputs": [],
      "source": [
        "#### extract all the zip files to unprocessed folder\n",
        "\n",
        "! unzip -o ./train.csv.zip -d ./unprocessed\n",
        "! unzip -o ./test.csv.zip -d ./unprocessed\n",
        "! unzip -o ./test_labels.csv.zip -d ./unprocessed\n",
        "\n",
        "clear_output(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faqNjZbCryqi"
      },
      "outputs": [],
      "source": [
        "englishStopWords = list(stopwords.words('english'))\n",
        "englishStopWords.extend(['utc','talk'])\n",
        "englishStopWords = set(englishStopWords)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "nDAY = r'(?:[0-3]?\\d)'  # day can be from 1 to 31 with a leading zero \n",
        "nMNTH = r'(?:11|12|10|0?[1-9])' # month can be 1 to 12 with a leading zero\n",
        "nYR = r'(?:(?:19|20)\\d\\d)'  # I've restricted the year to being in 20th or 21st century on the basis \n",
        "                            # that people doon't generally use all number format for old dates, but write them out \n",
        "nDELIM = r'(?:[\\/\\-\\._])?'  # \n",
        "NUM_DATE = f\"\"\"\n",
        "    (?P<num_date>\n",
        "        (?:^|\\D) # new bit here\n",
        "        (?:\n",
        "        # YYYY-MM-DD\n",
        "        (?:{nYR}(?P<delim1>[\\/\\-\\._]?){nMNTH}(?P=delim1){nDAY})\n",
        "        |\n",
        "        # YYYY-DD-MM\n",
        "        (?:{nYR}(?P<delim2>[\\/\\-\\._]?){nDAY}(?P=delim2){nMNTH})\n",
        "        |\n",
        "        # DD-MM-YYYY\n",
        "        (?:{nDAY}(?P<delim3>[\\/\\-\\._]?){nMNTH}(?P=delim3){nYR})\n",
        "        |\n",
        "        # MM-DD-YYYY\n",
        "        (?:{nMNTH}(?P<delim4>[\\/\\-\\._]?){nDAY}(?P=delim4){nYR})\n",
        "        )\n",
        "        (?:\\D|$) # new bit here\n",
        "    )\"\"\"\n",
        "DAY = r\"\"\"\n",
        "(?:\n",
        "    # search 1st 2nd 3rd etc, or first second third\n",
        "    (?:[23]?1st|2{1,2}nd|\\d{1,2}th|2?3rd|first|second|third|fourth|fifth|sixth|seventh|eighth|nineth)\n",
        "    |\n",
        "    # or just a number, but without a leading zero\n",
        "    (?:[123]?\\d)\n",
        ")\"\"\"\n",
        "MONTH = r'(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)'\n",
        "YEAR = r\"\"\"(?:(?:[12]?\\d|')?\\d\\d)\"\"\"\n",
        "DELIM = r'(?:\\s*(?:[\\s\\.\\-\\\\/,]|(?:of))\\s*)'\n",
        "\n",
        "YEAR_4D = r\"\"\"(?:[12]\\d\\d\\d)\"\"\"\n",
        "DATE_PATTERN = f\"\"\"(?P<wordy_date>\n",
        "    # non word character or start of string\n",
        "    (?:^|\\W)\n",
        "        (?:\n",
        "            # match various combinations of year month and day \n",
        "            (?:\n",
        "                # 4 digit year\n",
        "                (?:{YEAR_4D}{DELIM})?\n",
        "                    (?:\n",
        "                    # Day - Month\n",
        "                    (?:{DAY}{DELIM}{MONTH})\n",
        "                    |\n",
        "                    # Month - Day\n",
        "                    (?:{MONTH}{DELIM}{DAY})\n",
        "                    )\n",
        "                # 2 or 4 digit year\n",
        "                (?:{DELIM}{YEAR})?\n",
        "            )\n",
        "            |\n",
        "            # Month - Year (2 or 3 digit)\n",
        "            (?:{MONTH}{DELIM}{YEAR})\n",
        "        )\n",
        "    # non-word character or end of string\n",
        "    (?:$|\\W)\n",
        ")\"\"\"\n",
        "\n",
        "TIME = r\"\"\"(?:\n",
        "(?:\n",
        "# first number should be 0 - 59 with optional leading zero.\n",
        "[012345]?\\d\n",
        "# second number is the same following a colon\n",
        ":[012345]\\d\n",
        ")\n",
        "# next we add our optional seconds number in the same format\n",
        "(?::[012345]\\d)?\n",
        "# and finally add optional am or pm possibly with . and spaces\n",
        "(?:\\s*(?:a|p)\\.?m\\.?)?\n",
        ")\"\"\"\n",
        "\n",
        "COMBINED = f\"\"\"(?P<combined>\n",
        "    (?:\n",
        "        # time followed by date, or date followed by time\n",
        "        {TIME}?{DATE_PATTERN}{TIME}?\n",
        "        |\n",
        "        # or as above but with the numeric version of the date\n",
        "        {TIME}?{NUM_DATE}{TIME}?\n",
        "    ) \n",
        "    # or a time on its own\n",
        "    |\n",
        "    (?:{TIME})\n",
        ")\"\"\"\n",
        "\n",
        "regexDate = re.compile(COMBINED, re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
        "\n",
        "\n",
        "def removeNumber(comment):\n",
        "    return re.sub(r'\\d+',\"\",comment)\n",
        "\n",
        "def removeDoubleDots(comment):\n",
        "    return re.sub(r'\\.{2,}',\"\",comment)\n",
        "\n",
        "def removeDoubleSpace(comment):\n",
        "    return re.sub(r'[ ]{2,}',\" \",comment)\n",
        "\n",
        "def removePunctuations(comment):\n",
        "    return re.sub(r'[^\\w\\s]','',comment)\n",
        "\n",
        "def removeNewLine(comment):\n",
        "    return re.sub(r'\\n',\" \",comment)\n",
        "\n",
        "def removeURLs(comment):\n",
        "    return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', comment, flags=re.MULTILINE)\n",
        "\n",
        "def removeDate(comment):\n",
        "    return regexDate.sub(' ',comment)\n",
        "\n",
        "def removeRepeatingWords(comment):\n",
        "    return re.sub(r'(\\w+)( \\1)+', r'\\1', comment, flags=re.IGNORECASE)\n",
        "\n",
        "def lemmatize(comment):\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(comment) if word not in englishStopWords]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def processComment(comment):\n",
        "    comment = removeURLs(comment)\n",
        "    comment = removeDate(comment)\n",
        "    comment = removeNumber(comment)\n",
        "    comment = removeNewLine(comment)\n",
        "    comment = removeDoubleSpace(comment)\n",
        "    comment = removeDoubleDots(comment)\n",
        "    comment = removePunctuations(comment)\n",
        "    comment = lemmatize(comment)\n",
        "    comment = removeRepeatingWords(comment)\n",
        "    return comment.lower()    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextCleaner(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        return X.apply(lambda x : processComment(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ParagraphVectorizer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        dataset = './processed/w2vEmbeddings.model'\n",
        "        self.wv = KeyedVectors.load(dataset, mmap='r')\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        finalVector = []\n",
        "        for paragraph in X:\n",
        "            pVector = []\n",
        "            tokens = word_tokenize(paragraph)\n",
        "            for token in tokens:\n",
        "                if(self.wv.__contains__(token)):\n",
        "                    pVector.append(self.wv[token])\n",
        "            if len(pVector) > 0:\n",
        "                pVector = np.average(pVector,axis=0).tolist()\n",
        "            else:\n",
        "                pVector = np.zeros(100)\n",
        "            finalVector.append(pVector)\n",
        "        return pd.DataFrame(finalVector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PPgcK9sryqj"
      },
      "outputs": [],
      "source": [
        "# create pre processed data directory to optimize the preprocessing\n",
        "\n",
        "\n",
        "def mergeTrainCommentAndLabels(dfComment, dfLabels):\n",
        "    for index, row in dfComment.iterrows():\n",
        "        index = row.name\n",
        "        targetRow = dfLabels.iloc[index]\n",
        "        if row.id != targetRow.id:\n",
        "            print(\"Not matching\")\n",
        "        dfComment.at[index, 'toxic'] = int(targetRow.toxic)\n",
        "        dfComment.at[index, 'severe_toxic'] = int(targetRow.severe_toxic)\n",
        "        dfComment.at[index, 'obscene'] = int(targetRow.obscene)\n",
        "        dfComment.at[index, 'threat'] = int(targetRow.threat)\n",
        "        dfComment.at[index, 'insult'] = int(targetRow.insult)\n",
        "        dfComment.at[index, 'identity_hate'] = int(targetRow.identity_hate)\n",
        "        \n",
        "    finalDf = dfComment.copy(deep=True)\n",
        "    finalDf = finalDf.drop(finalDf[finalDf.toxic == -1].index)\n",
        "    finalDf['comment_text'] = finalDf['comment_text'].apply( lambda x: processComment(x))\n",
        "    return finalDf\n",
        "\n",
        "if not os.path.exists('./processed'):\n",
        "    os.makedirs('./processed')\n",
        "\n",
        "if not os.path.exists('./processed/train.csv'):\n",
        "    dataset = pd.read_csv('./unprocessed/train.csv')\n",
        "    dataset['comment_text'] = dataset['comment_text'].apply(\n",
        "        lambda x: processComment(x))\n",
        "    dataset.to_csv('./processed/train.csv')\n",
        "\n",
        "if not os.path.exists('./processed/test.csv'):\n",
        "    x = pd.read_csv('./unprocessed/test.csv')\n",
        "    y = pd.read_csv('./unprocessed/test_labels.csv')\n",
        "    finalDf = mergeTrainCommentAndLabels(x,y)\n",
        "    finalDf.to_csv('./processed/test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icZO7qVMryqk",
        "outputId": "da958d6f-b6db-4504-e739-6ecf7b105ba6"
      },
      "outputs": [],
      "source": [
        "trainData = pd.read_csv('./processed/train.csv')\n",
        "testData = pd.read_csv('./processed/test.csv')\n",
        "trainData = trainData.dropna()\n",
        "testData = testData.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(159343, 9)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainData.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(63875, 9)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testData.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5-bNLtpryql"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>explanation why edits made username hardcore m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>daww he match background colour im seemingly s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>hey man im really trying edit war its guy cons...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>more i cant make real suggestion improvement i...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>you sir hero any chance remember page thats</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                id  \\\n",
              "0           0  0000997932d777bf   \n",
              "1           1  000103f0d9cfb60f   \n",
              "2           2  000113f07ec002fd   \n",
              "3           3  0001b41b1c6bb37e   \n",
              "4           4  0001d958c54c6e35   \n",
              "\n",
              "                                        comment_text  toxic  severe_toxic  \\\n",
              "0  explanation why edits made username hardcore m...      0             0   \n",
              "1  daww he match background colour im seemingly s...      0             0   \n",
              "2  hey man im really trying edit war its guy cons...      0             0   \n",
              "3  more i cant make real suggestion improvement i...      0             0   \n",
              "4        you sir hero any chance remember page thats      0             0   \n",
              "\n",
              "   obscene  threat  insult  identity_hate  \n",
              "0        0       0       0              0  \n",
              "1        0       0       0              0  \n",
              "2        0       0       0              0  \n",
              "3        0       0       0              0  \n",
              "4        0       0       0              0  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>thank understanding i think highly would rever...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>dear god site horrible</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>somebody invariably try add religion reallyou ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>it say right is type the type institutioneeded...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>before adding new product list make surelevant...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                id  \\\n",
              "0           5  0001ea8717f6de06   \n",
              "1           7  000247e83dcc1211   \n",
              "2          11  0002f87b16116a7f   \n",
              "3          13  0003e1cccfd5a40a   \n",
              "4          14  00059ace3e3e9a53   \n",
              "\n",
              "                                        comment_text  toxic  severe_toxic  \\\n",
              "0  thank understanding i think highly would rever...    0.0           0.0   \n",
              "1                             dear god site horrible    0.0           0.0   \n",
              "2  somebody invariably try add religion reallyou ...    0.0           0.0   \n",
              "3  it say right is type the type institutioneeded...    0.0           0.0   \n",
              "4  before adding new product list make surelevant...    0.0           0.0   \n",
              "\n",
              "   obscene  threat  insult  identity_hate  \n",
              "0      0.0     0.0     0.0            0.0  \n",
              "1      0.0     0.0     0.0            0.0  \n",
              "2      0.0     0.0     0.0            0.0  \n",
              "3      0.0     0.0     0.0            0.0  \n",
              "4      0.0     0.0     0.0            0.0  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the word2vec embeddings\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "if not os.path.exists('./processed/w2vEmbeddings.model'):\n",
        "    word2vec = Word2Vec(\n",
        "        sentences=trainData['comment_text'].apply(\n",
        "            lambda x: nltk.word_tokenize(str(x))\n",
        "        ), \n",
        "        vector_size=100, \n",
        "        window=5, \n",
        "        min_count=1, \n",
        "        workers=10\n",
        "    )\n",
        "    word2vec.wv.save('./processed/w2vEmbeddings.model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vt55Bjqryql"
      },
      "outputs": [],
      "source": [
        "countVectorizerParams = {\n",
        "    # 'vectorizer__max_df': (0.5, 1.0),\n",
        "    # 'vectorizer__ngram_range': ((1, 1), (1, 2)),\n",
        "}\n",
        "\n",
        "tfidfVectorizerParams = {\n",
        "    # 'vectorizer__strip_accents' : ('ascii', 'unicode'),\n",
        "}\n",
        "\n",
        "logisticRegressionParams = {\n",
        "    # 'classifier__solver':['newton-cg', 'saga'],\n",
        "}\n",
        "\n",
        "mnbClassifierParams = {\n",
        "    # 'classifier__alpha' : [.2,.5,1],\n",
        "}\n",
        "\n",
        "rfClassifierParams = {\n",
        "    # 'classifier__max_depth' : [30,40],\n",
        "}\n",
        "\n",
        "dtClassifierParams = {\n",
        "    # 'classifier__splitter' : [\"best\", \"random\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La2ua_hzryqm"
      },
      "outputs": [],
      "source": [
        "tuples = {\n",
        "    \"transformers\": [\n",
        "        {\n",
        "            \"title\": \"TfIDF Vectorizer\",\n",
        "            \"tuples\": [\n",
        "                ('vectorizer', TfidfVectorizer(analyzer='word'))\n",
        "            ],\n",
        "            \"params\": {\n",
        "                **tfidfVectorizerParams,\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Count Vectorizer\",\n",
        "            \"tuples\": [\n",
        "                ('vectorizer', CountVectorizer(analyzer='word')),\n",
        "            ],\n",
        "            \"params\": {\n",
        "                **countVectorizerParams,\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Paragraph Vectorizer\",\n",
        "            \"tuples\": [\n",
        "                ('vectorizer', ParagraphVectorizer()),\n",
        "            ],\n",
        "            \"params\": {}\n",
        "        }\n",
        "    ],\n",
        "    \"classifiers\": [\n",
        "        # {\n",
        "        #     \"title\" : \"Multinomial Naive Bayes\",\n",
        "        #     \"tuples\" : [\n",
        "        #         ('scaler',MinMaxScaler()),\n",
        "        #         ('classifier',MultiOutputClassifier(MultinomialNB()))\n",
        "        #     ],\n",
        "        #     \"params\" : {\n",
        "        #         **mnbClassifierParams\n",
        "        #     }\n",
        "        # },\n",
        "        {\n",
        "            \"title\": \"Logistic Regression\",\n",
        "            \"tuples\": [\n",
        "                ('classifier', MultiOutputClassifier(LogisticRegression()))\n",
        "            ],\n",
        "            \"params\": {\n",
        "                **logisticRegressionParams\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Random Forest Classifier\",\n",
        "            \"tuples\": [\n",
        "                ('classifier', MultiOutputClassifier(RandomForestClassifier()))\n",
        "            ],\n",
        "            \"params\": {\n",
        "                **rfClassifierParams\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Decision Tree Classifier\",\n",
        "            \"tuples\": [\n",
        "                ('classifier', MultiOutputClassifier(DecisionTreeClassifier()))\n",
        "            ],\n",
        "            \"params\": {\n",
        "                **dtClassifierParams\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generateIdForRun(transformerName, classifierName):\n",
        "    name = \" \".join([transformerName, classifierName])\n",
        "    codeName = re.sub(r'[ ]+', \"_\", name)\n",
        "    codeName = codeName.lower()\n",
        "    return codeName\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7VbKikfryqm",
        "outputId": "56177047-761b-4d52-d1ee-3af8ee5877d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Running TfIDF Vectorizer + Logistic Regression\n",
            "TfIDF Vectorizer Logistic Regression is already trained\n",
            "=== Running TfIDF Vectorizer + Random Forest Classifier\n",
            "TfIDF Vectorizer Random Forest Classifier is already trained\n",
            "=== Running TfIDF Vectorizer + Decision Tree Classifier\n",
            "TfIDF Vectorizer Decision Tree Classifier is already trained\n",
            "=== Running Count Vectorizer + Logistic Regression\n",
            "Count Vectorizer Logistic Regression is already trained\n",
            "=== Running Count Vectorizer + Random Forest Classifier\n",
            "Count Vectorizer Random Forest Classifier is already trained\n",
            "=== Running Count Vectorizer + Decision Tree Classifier\n",
            "Count Vectorizer Decision Tree Classifier is already trained\n",
            "=== Running Paragraph Vectorizer + Logistic Regression\n",
            "Paragraph Vectorizer Logistic Regression is already trained\n",
            "=== Running Paragraph Vectorizer + Random Forest Classifier\n",
            "Paragraph Vectorizer Random Forest Classifier is already trained\n",
            "=== Running Paragraph Vectorizer + Decision Tree Classifier\n",
            "Paragraph Vectorizer Decision Tree Classifier is already trained\n"
          ]
        }
      ],
      "source": [
        "\n",
        "labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "best_models =[]\n",
        "\n",
        "for transformer in tuples[\"transformers\"]:\n",
        "    transformerName = transformer[\"title\"]\n",
        "    transformerTuple = transformer[\"tuples\"]\n",
        "    transformerParams = transformer[\"params\"]\n",
        "    for classifier in tuples[\"classifiers\"]:\n",
        "        classifierName = classifier[\"title\"]\n",
        "        classifierTuples = classifier[\"tuples\"]\n",
        "        classifierParams = classifier[\"params\"]\n",
        "\n",
        "        print(f'=== Running {transformerName} + {classifierName}')\n",
        "\n",
        "        finalParams = {\n",
        "            **transformerParams,\n",
        "            **classifierParams\n",
        "        }\n",
        "\n",
        "        pipeline = Pipeline(\n",
        "            transformerTuple\n",
        "            + classifierTuples\n",
        "        )\n",
        "\n",
        "        searchCV = GridSearchCV(\n",
        "            pipeline, \n",
        "            finalParams, \n",
        "            cv=3,\n",
        "            n_jobs=32, \n",
        "            verbose=1,\n",
        "            error_score='raise'\n",
        "        )\n",
        "\n",
        "        name = \" \".join([transformerName,classifierName])\n",
        "        idForRun = generateIdForRun(transformerName,classifierName)\n",
        "        \n",
        "\n",
        "        if not os.path.exists(f'./dumps'):\n",
        "            os.makedirs('./dumps')\n",
        "\n",
        "        if os.path.exists(f'./dumps/{idForRun}.bin'):\n",
        "            print(f\"{name} is already trained\")\n",
        "            continue\n",
        "\n",
        "        dataset = trainData.copy()\n",
        "\n",
        "        searchCV.fit(dataset['comment_text'],dataset[labels])\n",
        "\n",
        "        bestEstimator = searchCV.best_estimator_\n",
        "        bestScore = searchCV.best_score_\n",
        "        bestParams = searchCV.best_params_\n",
        "\n",
        "        best_models.append((f'{transformerName} & {classifierName}',bestEstimator,bestScore,bestParams))\n",
        "\n",
        "        print(f\"{name} score : {bestScore}\")\n",
        "\n",
        "        joblib.dump(bestEstimator,f'./dumps/{idForRun}.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "399hisbv7dJ7",
        "outputId": "eca21a6f-00d7-4aa3-c767-33959182e433"
      },
      "outputs": [],
      "source": [
        "def explain(name,score,params):\n",
        "    print(\"Search for CV \"+name)\n",
        "    print(\"Best score: %0.3f\" % score)\n",
        "    best_parameters = params\n",
        "    for param_name in sorted(params.keys()):\n",
        "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "\n",
        "for (name,estimator,score,param) in best_models:\n",
        "    explain(name,score,param)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcE9YupqWBgn",
        "outputId": "32725a6d-6b57-46d9-eea6-52675736c5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating graph for TfIDF Vectorizer Logistic Regression\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'ndim'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [34], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(outputDir)\n\u001b[1;32m     22\u001b[0m model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./dumps/\u001b[39m\u001b[39m{\u001b[39;00midForRun\u001b[39m}\u001b[39;00m\u001b[39m.bin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m plot_roc_curve(model,testData\u001b[39m.\u001b[39;49mcomment_text,testData[labels])\n\u001b[1;32m     26\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     27\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/deprecation.py:88\u001b[0m, in \u001b[0;36mdeprecated._decorate_fun.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fun)\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg, category\u001b[39m=\u001b[39m\u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_plot/roc_curve.py:451\u001b[0m, in \u001b[0;36mplot_roc_curve\u001b[0;34m(estimator, X, y, sample_weight, drop_intermediate, response_method, name, ax, pos_label, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39m\"\"\"Plot Receiver operating characteristic (ROC) curve.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m \u001b[39mExtra keyword arguments will be passed to matplotlib's `plot`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39m>>> plt.show()\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m check_matplotlib_support(\u001b[39m\"\u001b[39m\u001b[39mplot_roc_curve\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 451\u001b[0m y_pred, pos_label \u001b[39m=\u001b[39m _get_response(\n\u001b[1;32m    452\u001b[0m     X, estimator, response_method, pos_label\u001b[39m=\u001b[39;49mpos_label\n\u001b[1;32m    453\u001b[0m )\n\u001b[1;32m    455\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(\n\u001b[1;32m    456\u001b[0m     y,\n\u001b[1;32m    457\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    460\u001b[0m     drop_intermediate\u001b[39m=\u001b[39mdrop_intermediate,\n\u001b[1;32m    461\u001b[0m )\n\u001b[1;32m    462\u001b[0m roc_auc \u001b[39m=\u001b[39m auc(fpr, tpr)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_plot/base.py:105\u001b[0m, in \u001b[0;36m_get_response\u001b[0;34m(X, estimator, response_method, pos_label)\u001b[0m\n\u001b[1;32m    102\u001b[0m     class_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    103\u001b[0m     pos_label \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mclasses_[class_idx]\n\u001b[0;32m--> 105\u001b[0m \u001b[39mif\u001b[39;00m y_pred\u001b[39m.\u001b[39;49mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:  \u001b[39m# `predict_proba`\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     y_pred_shape \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m y_pred_shape \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'"
          ]
        }
      ],
      "source": [
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for transformer in tuples[\"transformers\"]:\n",
        "    transformerName = transformer[\"title\"]\n",
        "    for classifier in tuples[\"classifiers\"]:\n",
        "        classifierName = classifier[\"title\"]\n",
        "\n",
        "        name = f\"{transformerName} {classifierName}\"\n",
        "        title = f\"{transformerName} & {classifierName}\"\n",
        "        idForRun = generateIdForRun(transformerName, classifierName)\n",
        "        outputDir = f'./outputs/{name}'\n",
        "\n",
        "        print(f\"Generating graph for {name}\")\n",
        "\n",
        "        if not os.path.exists(f'./dumps/{idForRun}.bin'):\n",
        "            continue\n",
        "\n",
        "        if not os.path.exists(outputDir):\n",
        "            os.makedirs(outputDir)\n",
        "\n",
        "        model = joblib.load(f'./dumps/{idForRun}.bin')\n",
        "\n",
        "        \n",
        "        plot_roc_curve(model,testData.comment_text,testData[labels])\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "        # predictions = pd.DataFrame(model.predict(\n",
        "        #     testData.comment_text), columns=labels)\n",
        "\n",
        "        # cnfMatList = multilabel_confusion_matrix(testData[labels], predictions)\n",
        "        # report = classification_report(testData[labels], predictions,target_names=labels)\n",
        "        \n",
        "        \n",
        "\n",
        "        # for index, label in enumerate(labels):\n",
        "        #     columns = [\"Not \"+label,label]\n",
        "        #     df = pd.DataFrame(cnfMatList[index], columns, columns)\n",
        "        #     ax = plt.axes()\n",
        "        #     sn.heatmap(df, ax=ax, annot=True, cmap='Blues',fmt='d')\n",
        "        #     ax.set_title(title,pad=15)\n",
        "        #     plt.savefig(f'{outputDir}/{label}.png')\n",
        "        #     plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
